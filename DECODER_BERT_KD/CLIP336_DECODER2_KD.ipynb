{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl7yR7P3CTTj"
   },
   "source": [
    "### ライブラリの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7IQqCE7mCeD"
   },
   "source": [
    "###モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLEMbaPJOs9v",
    "outputId": "f47ad633-eb58-49cd-8893-914d53aa4475"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from typing import Sequence, Dict, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dataset\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import constant_, xavier_uniform_, kaiming_uniform_, xavier_normal_, kaiming_normal_\n",
    "\n",
    "from transformers import  CLIPVisionModel, get_linear_schedule_with_warmup, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "import util\n",
    "import levenshtein\n",
    "from nltk import bleu_score\n",
    "#from cidereval import cider, ciderD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        #positions = torch.arange(start=0, end=seq, step=1, device=x.device, requires_grad = False ).to(torch.long)\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device ).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siyThdM4Icll"
   },
   "source": [
    "### CaptioningTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcmR9lKrIbiL"
   },
   "outputs": [],
   "source": [
    "class CaptioningTransformer(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    null_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, img_size: int, length_max: int, dim_embedding: int, num_heads: int, \n",
    "                 prop_dec_num_layers: int, aux_dec_num_layers: int, vocab_size: int, tokenizer,\n",
    "                 dropout: float = 0.1, pad_token_id: int=0):\n",
    "        super().__init__()\n",
    "\n",
    "        #CLIP\n",
    "        model_id = \"openai/clip-vit-large-patch14-336\"\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(model_id, output_hidden_states = True)\n",
    "        images = torch.randn( ( 1, 3, img_size, img_size ) )\n",
    "        memory = self.clip_model( images )\n",
    "        memory = memory.last_hidden_state\n",
    "        img_length = memory.size(1)\n",
    "        clip_dim = memory.size(2)\n",
    "        \n",
    "        # Dense Connector\n",
    "\n",
    "        self.dc_linear = nn.Linear( clip_dim * 3, dim_embedding )\n",
    "        self.dropout = nn.Dropout( dropout )        \n",
    "        self.ln_memory = nn.LayerNorm( dim_embedding )\n",
    "\n",
    "        # Down Sampling\n",
    "        stride = img_length // ( length_max - 1 )\n",
    "        self.conv1 = nn.Conv1d( dim_embedding, dim_embedding, 1, stride )\n",
    "        print( \"img_length:\", img_length )\n",
    "        print( \"text_length_max:\", length_max )\n",
    "        print( \"stride:\", stride )\n",
    "        tgt = self.conv1( memory.transpose(1,2) ).transpose(1,2)\n",
    "        print( \"tgt size:\", tgt.size() )\n",
    "\n",
    "        \n",
    "        # proper decoder\n",
    "        self.prop_decoder_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model=dim_embedding, nhead=num_heads, batch_first=True, norm_first = True, activation='gelu')\n",
    "            for _ in range(prop_dec_num_layers)\n",
    "        ])\n",
    "\n",
    "        self.prop_ln = nn.LayerNorm( dim_embedding )\n",
    "        # 単語出力分布計算\n",
    "        self.prop_linear = nn.Linear(dim_embedding, vocab_size)        \n",
    "\n",
    "        # Word Embeidding\n",
    "        #self.emb = nn.Embedding( vocab_size, dim_embedding, padding_idx = tokenizer.pad_token_id )\n",
    "        # Word Embeidding\n",
    "        self.emb = nn.Embedding( vocab_size, dim_embedding )\n",
    "        \n",
    "        ## 位置エンコーディング\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        \n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # aux decoder\n",
    "        self.aux_decoder_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model=dim_embedding, nhead=num_heads, batch_first=True, norm_first = True, activation='gelu')\n",
    "            for _ in range(aux_dec_num_layers)\n",
    "        ])\n",
    "\n",
    "        self.aux_ln = nn.LayerNorm( dim_embedding )\n",
    "        # 単語出力分布計算\n",
    "        self.aux_linear = nn.Linear(dim_embedding, vocab_size)        \n",
    "        \n",
    "        # Others\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self._reset_parameters(self)\n",
    "\n",
    "    def _reset_parameters(self, model ):\n",
    "        for name, module in model.named_modules():\n",
    "            #print(\"name:\",name)\n",
    "            if \"clip_model\" not in name and \"prop_decoder\" in name:\n",
    "                if isinstance( module, nn.Linear ):\n",
    "                    #print(\"name1:\", name )\n",
    "                    for name, p in module.named_parameters():\n",
    "                        xavier_uniform_(module.weight)\n",
    "                        nn.init.zeros_(module.bias)\n",
    "                        #if name == \"weight\":\n",
    "                            #print( \"name2:\", name)\n",
    "                            #xavier_uniform_(p)\n",
    "                            #xavier_normal_(p)\n",
    "                            #kaiming_uniform_(p)\n",
    "                            #kaiming_normal_(p)\n",
    "                            #nn.init.normal_(p, mean=0.0, std=0.02)\n",
    "                        #elif name == \"bias\":\n",
    "                        #    nn.init.zeros_(p)\n",
    "                elif isinstance( module, nn.Embedding):\n",
    "                    nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                elif isinstance(module, nn.LayerNorm):\n",
    "                    nn.init.zeros_(module.bias)\n",
    "                    nn.init.ones_(module.weight)  \n",
    "            elif \"clip_model\" not in name and \"aux_decoder\" in name:\n",
    "                if isinstance( module, nn.Linear ):\n",
    "                    #print(\"name1:\", name )\n",
    "                    for name, p in module.named_parameters():\n",
    "                        xavier_uniform_(module.weight)\n",
    "                        nn.init.zeros_(module.bias)\n",
    "                        #if name == \"weight\":\n",
    "                            #print( \"name2:\", name)\n",
    "                            #xavier_uniform_(p)\n",
    "                            #xavier_normal_(p)\n",
    "                            #kaiming_uniform_(p)\n",
    "                            #kaiming_normal_(p)\n",
    "                            #nn.init.normal_(p, mean=0.0, std=0.02)\n",
    "                        #elif name == \"bias\":\n",
    "                        #    nn.init.zeros_(p)\n",
    "                elif isinstance( module, nn.Embedding):\n",
    "                    nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                elif isinstance(module, nn.LayerNorm):\n",
    "                    nn.init.zeros_(module.bias)\n",
    "                    nn.init.ones_(module.weight)  \n",
    "    \n",
    "            #if isinstance(module, nn.Linear):\n",
    "            #    nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #    if module.bias is not None:\n",
    "            #        nn.init.zeros_(module.bias)\n",
    "            #elif isinstance(module, nn.Embedding):\n",
    "            #    nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #elif isinstance(module, nn.LayerNorm):\n",
    "            #    nn.init.zeros_(module.bias)\n",
    "            #    nn.init.ones_(module.weight)  \n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "    '''\n",
    "    def forward(self, images: torch.Tensor, captions: torch.Tensor ):\n",
    "\n",
    "        self.device = images.device\n",
    "\n",
    "        memory = self.clip_model( images )\n",
    "        memory = self.dense_connector( memory )\n",
    "        memory = self.dropout( memory )\n",
    "        memory = self.ln_memory( memory )\n",
    "        aux_memory = memory # cross entropy 計算のため\n",
    "\n",
    "        tgt = self.conv1( memory.transpose(1,2) ).transpose(1,2)\n",
    "        \n",
    "        # ctc loss 計算のため\n",
    "        for layer in self.prop_decoder_layers:\n",
    "            tgt = layer( tgt, memory, tgt_mask = None, tgt_key_padding_mask = None, \n",
    "                       memory_key_padding_mask = None, tgt_is_causal = False )        \n",
    "            \n",
    "        # 単語出力分布計算\n",
    "        tgt = self.prop_ln( tgt )\n",
    "        prop_logits = self.prop_linear( tgt )\n",
    "\n",
    "        # cross entropy 計算のため    \n",
    "        aux_tgt = self.emb( captions ) * math.sqrt(self.dim_embedding)\n",
    "        \n",
    "        position = self.pos_emb( aux_tgt )\n",
    "        aux_tgt += position\n",
    "\n",
    "        #dropout\n",
    "        aux_tgt = self.dropout( aux_tgt )        \n",
    "            \n",
    "        #aux_tgt_key_padding_mask = torch.eq( captions, self.pad_token_id )\n",
    "        aux_tgt_key_padding_mask = None\n",
    "        #aux_tgt_mask = nn.Transformer.generate_square_subsequent_mask( captions.size(1), dtype=bool, device=self.device )\n",
    "        aux_tgt_mask = None\n",
    "        \n",
    "        for layer in self.aux_decoder_layers:\n",
    "            aux_tgt = layer( aux_tgt, aux_memory, tgt_mask = aux_tgt_mask, tgt_key_padding_mask = aux_tgt_key_padding_mask, \n",
    "                       memory_key_padding_mask = None, tgt_is_causal = False )        \n",
    "                       #memory_key_padding_mask = None, tgt_is_causal = True )        \n",
    "        # 単語出力分布計算\n",
    "        aux_tgt = self.aux_ln( aux_tgt )\n",
    "        aux_logits = self.aux_linear( aux_tgt )\n",
    "            \n",
    "        return prop_logits, aux_logits\n",
    "\n",
    "    def dense_connector(self, memory ):\n",
    "        tmp1 = torch.tensor([], device = self.device  )\n",
    "        tmp2 = torch.tensor([], device = self.device  )\n",
    "        tmp_full = len( memory.hidden_states )\n",
    "        tmp_half = tmp_full // 2\n",
    "        for i in range( 0, tmp_half ):\n",
    "            tmp1 = torch.cat( [tmp1, memory.hidden_states[i][None]], dim = 0 )\n",
    "        tmp1 = torch.sum(tmp1, dim=0) / tmp_half\n",
    "        for i in range( tmp_half, tmp_full ):\n",
    "            tmp2 = torch.cat( [tmp2, memory.hidden_states[i][None]], dim = 0 )\n",
    "        tmp2 = torch.sum(tmp2, dim=0 ) / ( tmp_full - tmp_half )\n",
    "        tmp3 = torch.cat([tmp1, tmp2], dim=-1)\n",
    "        tmp3 = torch.cat( [ memory.last_hidden_state, tmp3], dim = -1 )\n",
    "        #tmp3 = sel.dc_ln( tmp3 )\n",
    "        tmp3 = self.dc_linear( tmp3 )\n",
    "        return tmp3\n",
    "    '''\n",
    "    def dense_connector(self, memory ):\n",
    "        tmp = torch.zeros_like( memory.hidden_states[0] )\n",
    "        for tmp1 in memory.hidden_states:\n",
    "            tmp += tmp1\n",
    "        tmp += memory.last_hidden_state\n",
    "        tmp /= (len( memory.hidden_states) + 1 )\n",
    "\n",
    "        tmp = self.dc_linear( tmp )\n",
    "\n",
    "        return tmp\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path: str, img_directory: str, transforms, tokenizer, length_max = None ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_directory = img_directory\n",
    "        self.transforms = transforms\n",
    "        # TODO: fix to original data\n",
    "        #画像の前処理\n",
    "        self.img_file = []\n",
    "        self.tokens = []\n",
    "        #vocab_size = len( tokenizer )\n",
    "        #c1 = torch.zeros( ( vocab_size ) )\n",
    "        #c2 = torch.zeros( ( vocab_size, vocab_size ) )\n",
    "        if length_max == None:\n",
    "            self.length_max = 0\n",
    "        else:\n",
    "            self.length_max = length_max\n",
    "        length_sum = 0\n",
    "\n",
    "        with open( file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for i, line_data in enumerate( data ):\n",
    "            if i % 100000 == 0:\n",
    "                print( \"i:\", i )\n",
    "            self.img_file.append( line_data['img_file'] )\n",
    "            id_tokens = line_data['id_tokens']\n",
    "            length_sum += len( id_tokens )\n",
    "            if length_max != None:\n",
    "                id_tokens = torch.tensor( id_tokens )[:self.length_max]\n",
    "            else:\n",
    "                if self.length_max < len( id_tokens ):\n",
    "                    self.length_max = len( id_tokens )\n",
    "                id_tokens = torch.tensor( id_tokens )\n",
    "            self.tokens.append( id_tokens )\n",
    "        # w1, w2 を作る時は length_max = None　でお願いします。\n",
    "        #    for i2 in range( len(id_tokens) ):\n",
    "        #        if i2 == len( id_tokens ) - 1:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #        else:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #            c2[id_tokens[i2], id_tokens[i2+1] ] += 1\n",
    "        '''\n",
    "        c1avg = int( torch.sum( c1 ) / torch.sum( torch.ne( c1, 0 ).int()) )\n",
    "        c2avg = int( torch.sum( torch.sum( c2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( c2, 0 ).int() ) )\n",
    "\n",
    "        c1[0] = c1avg\n",
    "\n",
    "        c2[:,0] = c2avg\n",
    "        c2[0,:] = c2avg\n",
    "        \n",
    "        sumc1 = torch.sum( c1, dim = 0 )\n",
    "        sumc2 = torch.sum( torch.sum( c2, dim = 1 ), dim = 0 )\n",
    "\n",
    "        prob1 = c1 / sumc1\n",
    "        prob2 = c2 / sumc2\n",
    "\n",
    "        self.w1 = prob1 ** -0.4\n",
    "        self.w1 = torch.nan_to_num( self.w1, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg1 = torch.sum( self.w1, dim = 0 ) / torch.sum( torch.ne( self.w1, 0.0 ).int() )\n",
    "        self.w1 = self.w1 / avg1\n",
    "\n",
    "        self.w2 = prob2 ** -0.4\n",
    "        self.w2 = torch.nan_to_num( self.w2, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg2 = torch.sum( torch.sum( self.w2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( self.w2, 0.0 ).int() )\n",
    "        self.w2 = self.w2 / avg2\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_unigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w1, f )\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_bigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w2, f )\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_unigram.pkl\", 'rb') as f:\n",
    "        #    self.w1 = pickle.load(f)\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_bigram.pkl\", 'rb') as f:\n",
    "        #    self.w2 = pickle.load(f)\n",
    "        \n",
    "        if length_max == None:\n",
    "            print( \"length max:\", self.length_max )\n",
    "            print( \"avg length:\", length_sum / len( self.tokens ) )\n",
    "    \n",
    "    # ここで取り出すデータを指定している\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        tokens = self.tokens[index]\n",
    "        img_file = self.img_file[index] + \".jpg\"\n",
    "        img_path = os.path.join( self.img_directory, img_file ) #index番目の画像のパスを取得\n",
    "        img = Image.open(img_path) #PIL形式で画像を読み込み\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        \n",
    "        return img, tokens\n",
    "\n",
    "    # この method がないと DataLoader を呼び出す際にエラーを吐かれる\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def length_max(self):\n",
    "        return self.length_max\n",
    "\n",
    "    #def w1(self):\n",
    "    #    return self.w1\n",
    "\n",
    "    #def w2(self):\n",
    "    #    return self.w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, str]]], pad_index, length_max ):\n",
    "    imgs, tokens = zip(*batch)\n",
    "\n",
    "    max_length = length_max\n",
    "    #max_length = 0\n",
    "    #for target in tokens:\n",
    "    #    if max_length < len( target ):\n",
    "    #        max_length = len( target )\n",
    "    \n",
    "    targets = []\n",
    "    lengths = []\n",
    "    for target in tokens:\n",
    "        pad_len = max_length - len( target ) \n",
    "        #print( \"target:\", target )\n",
    "        input2= F.pad( target, (0, pad_len), mode='constant', value = pad_index)\n",
    "        targets.append( input2 )\n",
    "        lengths.append( len( target ) )\n",
    "    \n",
    "    imgs = torch.stack( imgs, dim = 0 )\n",
    "    targets = torch.stack( targets, dim = 0 )\n",
    "    lengths = torch.tensor( lengths, requires_grad = False  )\n",
    "\n",
    "    #if imgs.dim() != 4:\n",
    "    #   print( \"in collate imgs size:\", imgs.size() )\n",
    "    \n",
    "    return imgs, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"../CLIP_ENCODER/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b\"\n",
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = path)\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "blank_token_id = tokenizer.encode( [ \"[unused0]\"] )[1]\n",
    "print( \"blank token_id:\", blank_token_id )\n",
    "\n",
    "\n",
    "caption = \"Hello World!\".lower()\n",
    "token_ids = tokenizer.encode( caption )\n",
    "print( token_ids )\n",
    "#decoded = tokenizer.tokenize( token_ids )\n",
    "#print( decoded )\n",
    "decoded = tokenizer.decode( token_ids )\n",
    "print( decoded )\n",
    "#print( tokenizer.tokenize( decoded ) )\n",
    "#individual_decoded_tokens = list(map(tokenizer.decode, token_ids))\n",
    "individual_decoded_tokens = [tokenizer.decode( [token], skip_special_tokens = True ) for token in token_ids ]\n",
    "individual_decoded_tokens2 = [tokens for tokens in individual_decoded_tokens if tokens != '' ]\n",
    "print( individual_decoded_tokens2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device( 'cpu' )\n",
    "#path = \"../CLIP_ENCODER/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b\"\n",
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = path)\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "blank_token_id = tokenizer.encode( [ \"[unused0]\"] )[1]\n",
    "print( \"blank  token_id:\", blank_token_id )\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id)\n",
    "\n",
    "#path = \"../CLIP_ENCODER/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b\"\n",
    "#kd_model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path = path).to(config.device)\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "kd_model = BertForMaskedLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((336, 336)),\n",
    "    #v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.AutoAugment(),\n",
    "    ## Coco データセット 2017 train の平均と標準偏差\n",
    "    #v2.Normalize((0.456,0.427,0.401),(0.224,0.219,0.231) )\n",
    "    # Clip Model の config から引用。\n",
    "    v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "#transforms = T.Compose([\n",
    "#    T.Resize((336, 336)),\n",
    "#    T.RandomHorizontalFlip(),\n",
    "#    T.ToTensor(),\n",
    "#    # ImageNetデータセットの平均と標準偏差\n",
    "#    T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "#])\n",
    "\n",
    "\n",
    "# v7 データセット\n",
    "train_dataset = MyDataset( file_path=\"/mnt/ssd2/v7/data.pkl\",\n",
    "                           img_directory = \"/mnt/ssd2/v7/img\",\n",
    "                           #img_directory = \"smb://192.168.1.2/img/v7/\",\n",
    "                           transforms=transforms, tokenizer = tokenizer, length_max = 97 )\n",
    "\n",
    "print( \"train_dataset.length_max:\", train_dataset.length_max )\n",
    "\n",
    "# Subset samplerの生成\n",
    "#val_set, train_set = util.generate_subset(\n",
    "#    train_dataset, config.val_ratio)\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, 0.1, 0.1 )\n",
    "    \n",
    "# 学習時にランダムにサンプルするためのサンプラー\n",
    "train_sampler = SubsetRandomSampler(train_set)\n",
    "\n",
    "# DataLoaderを生成\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id, length_max = 97 )\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=4,\n",
    "                    num_workers=0,\n",
    "                    shuffle = False,\n",
    "                    #sampler=train_sampler,\n",
    "                    pin_memory = True,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "transforms_inv = v2.Compose([\n",
    "    v2.Normalize((-0.48145466/0.26862954, -0.4578275/0.26130258, -0.40821073/0.27577711), (1/0.26862954,1/0.26130258,1/0.27577711)),\n",
    "    v2.ToPILImage()\n",
    "])\n",
    "\n",
    "\n",
    "#for n, (imgs, targets, lengths) in enumerate( train_loader ):\n",
    "#    #print( \"imgs size:\", imgs.size())\n",
    "#    if n % 100 == 0:\n",
    "#        print( \"n:\", n )\n",
    "    \n",
    "imgs, targets, lengths = next(iter(train_loader))\n",
    "\n",
    "##print( targets )\n",
    "print( targets.size() )\n",
    "print( imgs.size() )\n",
    "##print( imgs[0] )\n",
    "##img[0].show()\n",
    "inv_img = transforms_inv( imgs[0] )\n",
    "plt.imshow( inv_img )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4x-PO05mCS-"
   },
   "source": [
    "###学習におけるハイパーパラメータやオプションの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQES3A8OG-V_"
   },
   "outputs": [],
   "source": [
    "class ConfigTrain(object):\n",
    "    '''\n",
    "    ハイパーパラメータ、システム共通変数の設定\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        # ハイパーパラメータ\n",
    "        #self.clip_dim = 1024\n",
    "        self.img_size = 336\n",
    "        self.dim_embedding = 1024   # 埋め込み層の次元\n",
    "        self.num_heads = 16\n",
    "        #self.num_heads = 20\n",
    "        #self.prop_dec_num_layers = 12\n",
    "        self.prop_dec_num_layers = 24\n",
    "        #self.aux_dec_num_layers = 12\n",
    "        self.aux_dec_num_layers = 24\n",
    "        self.length_max = 97\n",
    "        #self.lr = 5e-5            # 学習率\n",
    "        #self.lr = 2e-5            # 学習率\n",
    "        #self.lr = 1e-5            # 学習率\n",
    "        #self.lr = 2e-6            # 学習率\n",
    "        #self.lr = 1e-6            # 学習率\n",
    "        #self.lr = 5e-7            # 学習率\n",
    "        #self.lr_clip = 2e-7            # 学習率\n",
    "        self.lr_clip = 2e-7            # 学習率\n",
    "        self.lr_prop_decoder = 1e-4            # 学習率\n",
    "        self.lr_aux_decoder = 1e-4            # 学習率\n",
    "        self.lr_others = 1e-4\n",
    "        self.dropout = 0.1\n",
    "        #self.batch_size = 128       # ミニバッチ数\n",
    "        #self.batch_size = 24       # ミニバッチ数\n",
    "        #self.batch_size = 16       # ミニバッチ数\n",
    "        self.batch_size = 12       # ミニバッチ数\n",
    "        #self.batch_size = 8       # ミニバッチ数\n",
    "        #self.batch_size = 6       # ミニバッチ数\n",
    "        #self.batch_size = 4       # ミニバッチ数\n",
    "        #self.batch_size = 2       # ミニバッチ数\n",
    "        #self.batch_size = 1       # ミニバッチ数\n",
    "        #self.num_epochs = 100       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        #self.num_epochs = 60       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        self.num_epochs = 10       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        #self.num_epochs = 5       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        self.use_amp = True\n",
    "        #self.use_amp = False\n",
    "        #self.use_saved_pth = True\n",
    "        self.use_saved_pth = False\n",
    "        self.vocab_size = len( tokenizer )\n",
    "        self.alpha = 0.3\n",
    "        self.T = 1.0\n",
    "        self.weight_decay = 0.1\n",
    "        self.betas = (0.9, 0.999 )\n",
    "        self.warmup = 0.1\n",
    "\n",
    "        # パスの設定\n",
    "        self.img_directory = '/mnt/ssd2/v7/img'\n",
    "        self.anno_file = '/mnt/ssd2/v7/data.pkl'\n",
    "        self.save_directory = './model'\n",
    "\n",
    "        # 検証に使う学習セット内のデータの割合\n",
    "        self.test_ratio = 0.1\n",
    "        self.val_ratio = 0.1\n",
    "        #self.val_ratio = 0.0004\n",
    "        #self.test_ratio = 0.0004\n",
    "        \n",
    "        # 学習に使うデバイス\n",
    "        #self.device = 'cuda'\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = 'cpu'\n",
    "        \n",
    "        # データローダーに使うCPUプロセスの数\n",
    "        #self.num_workers = 4\n",
    "        self.num_workers = 0 if self.device == torch.device('cpu') else 8\n",
    "        #self.num_workers = 0 if self.device == torch.device('cpu') else 6\n",
    "        #self.num_workers = 0\n",
    "        \n",
    "        # 移動平均で計算する損失の値の数\n",
    "        self.moving_avg = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "#path = \"../CLIP_ENCODER/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b\"\n",
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = path)\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "blank_token_id = tokenizer.encode( [ \"[unused0]\"] )[1]\n",
    "print( \"blank token_id:\", blank_token_id )\n",
    "model = CaptioningTransformer( img_size = 336, length_max = train_dataset.length_max, dim_embedding=1280, num_heads = 16, \\\n",
    "                               prop_dec_num_layers = 12, aux_dec_num_layers=12, vocab_size=len(tokenizer),tokenizer=tokenizer, \\\n",
    "                               dropout = 0.1, pad_token_id = tokenizer.pad_token_id ).to(device)\n",
    "images = torch.randn( ( 2, 3, 336,336 ), device = device )\n",
    "captions = torch.randint( 0, len(tokenizer), size=( 2, 50  ), device = device )\n",
    "prop_logits, aux_logits = model( images, captions[:,:-1] )\n",
    "\n",
    "print( prop_logits.size() )\n",
    "print( aux_logits.size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_simple_decode(int_vector, tokenizer):\n",
    "    ''' 以下の手順で，フレーム単位のCTC出力をトークン列に変換する\n",
    "        1. 同じ文字が連続して出現する場合は削除\n",
    "        2. blank を削除\n",
    "    int_vector: フレーム単位のCTC出力(整数値列)\n",
    "    token_list: トークンリスト\n",
    "    output:     トークン列\n",
    "    '''\n",
    "    # 出力文字列\n",
    "    output = []\n",
    "    # 一つ前フレームの文字番号\n",
    "    prev_token = -1\n",
    "    # フレーム毎の出力文字系列を前から順番にチェックしていく\n",
    "    for n in int_vector:\n",
    "        n = n.item()\n",
    "        if n != prev_token:\n",
    "            # 1. 前フレームと同じトークンではない\n",
    "            if n != tokenizer.encode( [ \"[unused0]\"] )[1] and n != tokenizer.encode( [ \"[CLS]\"] )[1] \\\n",
    "                and n != tokenizer.encode( [ \"[SEP]\"] )[1] and n != tokenizer.encode( [ \"[PAD]\"] )[1]:\n",
    "                # 2. かつ，blank(番号=0)ではない\n",
    "                # --> token_listから対応する文字を抽出し，\n",
    "                #     出力文字列に加える\n",
    "                output.append( tokenizer.decode( [n] ) )\n",
    "            # 前フレームのトークンを更新\n",
    "            prev_token = n\n",
    "        if n == tokenizer.encode( [\"[SEP]\"] )[1]:\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--NNWCwZI5qS"
   },
   "source": [
    "### 学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBOP-3aIHFjB"
   },
   "outputs": [],
   "source": [
    "#path = \"../CLIP_ENCODER/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b\"\n",
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = path)\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "blank_token_id = tokenizer.encode( [ \"[unused0]\"] )[1]\n",
    "print( \"blank token_id:\", blank_token_id )\n",
    "\n",
    "config = ConfigTrain()\n",
    "\n",
    "print( \"use_amp:\", config.use_amp )\n",
    "\n",
    "#path = \"../CLIP_ENCODER/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b\"\n",
    "#kd_model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path = path).to(config.device)\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "kd_model = BertForMaskedLM.from_pretrained(model_id).to(config.device)\n",
    "\n",
    "# モデル出力用のディレクトリを作成\n",
    "os.makedirs(config.save_directory, exist_ok=True)\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((config.img_size, config.img_size)),\n",
    "    #v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.AutoAugment(),\n",
    "    ## Coco データセット 2017 train の平均と標準偏差\n",
    "    #v2.Normalize((0.456,0.427,0.401),(0.224,0.219,0.231) )\n",
    "    # Clip Model の config から引用。\n",
    "    v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# v7 データセット\n",
    "train_dataset = MyDataset( file_path=config.anno_file,\n",
    "                           img_directory = config.img_directory,\n",
    "                           transforms=transforms,tokenizer=tokenizer, length_max = config.length_max)\n",
    "\n",
    "# Subset samplerの生成\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, config.test_ratio, config.val_ratio )\n",
    "    \n",
    "# 学習時にランダムにサンプルするためのサンプラー\n",
    "train_sampler = SubsetRandomSampler(train_set)\n",
    "\n",
    "# DataLoaderを生成\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id, config.length_max)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=train_sampler,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=val_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    #batch_size=config.batch_size,\n",
    "                    batch_size=1,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=test_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "\n",
    "print( \"device:\", config.device )\n",
    "print( \"学習セット数:\",len( train_loader ) )\n",
    "print( \"評価セット数:\",len( val_loader ))\n",
    "print( \"テストセット数:\",len( test_loader ))\n",
    "    \n",
    "# モデルの定義\n",
    "model = CaptioningTransformer( config.img_size, config.length_max, config.dim_embedding, config.num_heads, \n",
    "                               config.prop_dec_num_layers, config.aux_dec_num_layers, \n",
    "                               len(tokenizer), tokenizer, config.dropout, tokenizer.pad_token_id)\n",
    "model = model.to(config.device) \n",
    "\n",
    "# 損失関数の定義\n",
    "ctc_criterion = nn.CTCLoss(blank=blank_token_id, reduction='mean',zero_infinity=True) \n",
    "#ce_criterion = nn.CrossEntropyLoss( ignore_index = tokenizer.pad_token_id, reduction = 'mean' )\n",
    "#ce_criterion = nn.CrossEntropyLoss( reduction = 'mean' )\n",
    "\n",
    "\n",
    "# 最適化手法の定義\n",
    "# Optimizerの生成, clipとそうでないモジュールとの\n",
    "# パラメータで異なる学習率を適用\n",
    "params_clip = []\n",
    "params_prop_decoder = []\n",
    "params_aux_decoder = []\n",
    "params_others = []\n",
    "for name, parameter in model.named_parameters():\n",
    "    if parameter.requires_grad:\n",
    "        if 'clip_model' in name:\n",
    "            params_clip.append(parameter)\n",
    "        elif 'prop_decoder' in name:\n",
    "            params_prop_decoder.append(parameter)\n",
    "        elif 'aux_decoder'in name:\n",
    "            params_aux_decoder.append(parameter)\n",
    "        else:\n",
    "            params_others.append(parameter)\n",
    "param_groups = [\n",
    "    {'params': params_clip, 'lr': config.lr_clip},\n",
    "    {'params': params_prop_decoder, 'lr': config.lr_prop_decoder},\n",
    "    {'params': params_aux_decoder, 'lr': config.lr_aux_decoder},\n",
    "    {'params': params_others, 'lr': config.lr_others}]\n",
    "optimizer = torch.optim.AdamW( param_groups, weight_decay = config.weight_decay, betas=config.betas )\n",
    "\n",
    "# 全ステップ数\n",
    "num_global_steps = len( train_loader ) * config.num_epochs\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "num_warmup_steps = num_global_steps * 0.1\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "#スケジューラーの定義\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps )    \n",
    "\n",
    "PATH = f'{config.save_directory}/model_clip_decoder2_kd_nar_curr.pth'\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "print( \"exist saved_pth:\", os.path.isfile(PATH) ) \n",
    "if config.use_saved_pth and os.path.isfile(PATH):\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    ##device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "    #for state in optimizer.state.values():\n",
    "        #for k, v in state.items():\n",
    "            #if isinstance(v, torch.Tensor):\n",
    "                #state[k] = v.to(device)\n",
    "    begin_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    global_step = checkpoint['global_step']\n",
    "else:\n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "print( \"begin_epoch:\", begin_epoch )\n",
    "print( \"global_step:\", global_step )\n",
    "    \n",
    "len_tr_loader = len( train_loader )\n",
    "train_param = len_tr_loader // 3\n",
    "len_val_loader = len( val_loader )\n",
    "#train_param = len_val_loader // 3\n",
    "val_param = len_val_loader // 3\n",
    "print( \"train_param:\", train_param )\n",
    "print( \"val_param:\", val_param )\n",
    "\n",
    "print( \"epochs:\", config.num_epochs )\n",
    "print( \"batch_size:\", config.batch_size )\n",
    "print( \"lr_clip:\", config.lr_clip )\n",
    "print( \"lr_prop_decoder:\", config.lr_prop_decoder )\n",
    "print( \"lr_aux_decoder:\", config.lr_aux_decoder )\n",
    "print( \"lr_others:\", config.lr_others )\n",
    "print( \"weight_decay:\", config.weight_decay )\n",
    "print( \"betas:\", config.betas )\n",
    "print( \"alpha:\", config.alpha )\n",
    "print( \"T:\", config.T )\n",
    "\n",
    "# 学習経過の書き込み\n",
    "now = datetime.datetime.now()\n",
    "train_loss_file = '{}/MyOriginal_train_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(train_loss_file, 'a') as f:\n",
    "    print(f'{len_tr_loader}', file=f)\n",
    "print( \"train_loss_file:\", train_loss_file )\n",
    "val_loss_file = '{}/MyOriginal_val_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(val_loss_file, 'a') as f:\n",
    "    print(f'{len_val_loader}', file=f) \n",
    "norm_file = '{}/norm_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "# 学習\n",
    "val_loss_best = float('inf')\n",
    "\n",
    "fn = bleu_score.SmoothingFunction().method7\n",
    "\n",
    "# AMP用のスケーラー\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "for epoch in range(begin_epoch, config.num_epochs):\n",
    "    with tqdm(train_loader) as pbar:\n",
    "    #with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[エポック {epoch + 1}]')\n",
    "\n",
    "        # 学習モードに設定\n",
    "        model.train()\n",
    "\n",
    "        train_losses = deque()\n",
    "        train_errors = deque()\n",
    "        train_bleus = deque()\n",
    "        #train_ciders = deque()\n",
    "        for n_batch, (imgs, captions, caption_lengths) in enumerate( pbar ):\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "            caption_lengths = caption_lengths.to(config.device)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(str(config.device),enabled=config.use_amp):\n",
    "                # 最後の単語から次を予測する必要はないため最後の単語を除外\n",
    "                ctc_logits, ce_logits = model(imgs, captions )\n",
    "                hypo_ids = torch.argmax( ctc_logits, dim = 2 )\n",
    "                ctc_outputs_lengths = torch.full( size = (ctc_logits.size(0), ), fill_value = ctc_logits.size(1), dtype=torch.long)\n",
    "                ctc_outputs = F.log_softmax( ctc_logits, dim=2 )\n",
    "                \n",
    "                # 損失の計算\n",
    "                # 単語軸が第1軸である必要があるため、転置\n",
    "                ctc_loss = ctc_criterion(ctc_outputs.transpose(0, 1),captions,ctc_outputs_lengths,caption_lengths)\n",
    "                #loss = config.alpha * ctc_loss + ( 1 - config.alpha ) * ce_loss\n",
    "                with torch.no_grad():\n",
    "                    #attention_mask = None\n",
    "                    #kd_logits = kd_model( input_ids = captions, attention_mask = attention_mask )[0]\n",
    "                    kd_logits = kd_model( input_ids = captions )[0]\n",
    "                kd_loss = nn.KLDivLoss()(F.log_softmax(ce_logits/config.T, dim=-1),F.softmax(kd_logits/config.T, dim=-1)) * ( config.T * config.T )\n",
    "                loss = config.alpha * ctc_loss + ( 1 - config.alpha ) * kd_loss\n",
    "\n",
    "            # 誤差逆伝播\n",
    "            scaler.scale(loss).backward()\n",
    "            #scaler.unscale_(optimizer)\n",
    "            #clip_grad_threshold = 5.0\n",
    "            #torch.nn.utils.clip_grad_norm_(\\\n",
    "            #        model.parameters(),\n",
    "            #        clip_grad_threshold)\n",
    "            # オプティマイザにより，パラメータを更新する\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()            \n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            #for name, param in model.named_parameters():\n",
    "            #    print( name )\n",
    "            # 勾配消失が起こっていないか監視\n",
    "            norm0 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[0].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            norm1 = torch.sqrt( torch.norm( model.prop_decoder_layers[23].self_attn.in_proj_weight.grad, p = 2 ) ).item()\n",
    "            norm_mean = torch.mean( torch.stack ([ torch.sqrt( torch.norm( param.grad, p = 2 ) ) \\\n",
    "                                                  for param in model.parameters() if param.grad is not None ] ) ).item()\n",
    "            with open(norm_file, 'a') as f:\n",
    "                print( \"epcoch:\", epoch, \", step:\", global_step, \", norm0:\", norm0, \", norm1:\", norm1, \", norm_mean:\", norm_mean, file=f  )\n",
    "                f.flush()\n",
    "            global_step += 1\n",
    "\n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            #total_cider = 0\n",
    "            n2 = 0\n",
    "            for n3, (hypo_id, caption) in enumerate( zip( hypo_ids, captions )):\n",
    "                hypo_tokens = ctc_simple_decode(hypo_id, tokenizer)\n",
    "                hypo = ' '.join( hypo_tokens )\n",
    "                reference = tokenizer.decode( caption.tolist(), skip_special_tokens = True )\n",
    "                ref_tokens = tokenizer.tokenize( reference )\n",
    "                \n",
    "                # 認識誤りを計算\n",
    "                (error, substitute, delete, insert, ref_length) = levenshtein.calculate_error(hypo_tokens,ref_tokens)\n",
    "                # 誤り文字数を累積する\n",
    "                total_error += error\n",
    "                # 文字の総数を累積する\n",
    "                total_token_length += ref_length\n",
    "\n",
    "                bleu = bleu_score.sentence_bleu( [reference], hypo, smoothing_function=fn  )\n",
    "                #cider_ = cider(predictions=[hypo], references=[[reference]])['avg_score']\n",
    "        \n",
    "                total_bleu += bleu\n",
    "                #total_cider += cider_\n",
    "                \n",
    "                if n < 1 and n_batch == len( train_loader ) - 1 :\n",
    "                #if n < 1 and n_batch == len( val_loader ) - 1 :\n",
    "                    hypo_sentence.append( hypo )\n",
    "                    ref_sentence.append( reference )\n",
    "                if n < 1 and n_batch % train_param == 0:\n",
    "                    hypo_sentence1.append( hypo )\n",
    "                    ref_sentence1.append( reference )\n",
    "                    \n",
    "                n += 1\n",
    "                n2 += 1\n",
    "            \n",
    "            avg_error = total_error / total_token_length * 100\n",
    "            avg_bleu = total_bleu / n2 * 100\n",
    "            #avg_cider = total_cider / n2\n",
    "            \n",
    "            # 学習時の損失をログに書き込み\n",
    "            train_losses.append(loss.item())\n",
    "            train_errors.append( avg_error )\n",
    "            train_bleus.append( avg_bleu )\n",
    "            #train_ciders.append( avg_cider )\n",
    "            if len(train_losses) > config.moving_avg:\n",
    "                train_losses.popleft()\n",
    "                train_errors.popleft()\n",
    "                train_bleus.popleft()\n",
    "                #train_ciders.popleft()\n",
    "            mean_loss = torch.Tensor(train_losses).mean().item()\n",
    "            mean_error = torch.Tensor(train_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(train_bleus).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                'loss': mean_loss,\n",
    "                'WER': mean_error,\n",
    "                'BLEU': mean_bleu,\n",
    "                #'CIDER': torch.Tensor(train_ciders).mean().item()\n",
    "            })\n",
    "            with open(train_loss_file, 'a') as f:\n",
    "                #print(f'{epoch}, {loss.item()},  {avg_error}, {avg_bleu}, {avg_cider}', file=f)\n",
    "                print(f'{epoch}, {mean_loss}, {mean_error}, {mean_bleu}', file=f)\n",
    "            print_flag = 1\n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                if print_flag == 1:\n",
    "                    print( \"lr_clip  :\", optimizer.param_groups[0][\"lr\"] )\n",
    "                    print( \"lr_prop_decoder:\", optimizer.param_groups[1][\"lr\"] )\n",
    "                    print( \"lr_aux_decoder:\", optimizer.param_groups[2][\"lr\"] )\n",
    "                    print( \"lr_others:\", optimizer.param_groups[3][\"lr\"] )\n",
    "                    print_flag = 0\n",
    "                #print(f'Train epoch = {epoch}, loss = {loss.item()}, WER = {avg_error}, BLEU = {avg_bleu}, CIDER = {avg_cider}')\n",
    "                print(f'Train epoch = {epoch}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "                    \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence, ref_sentence ):\n",
    "                print(f'Train epoch = {epoch}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                #print(f'Train epoch = {epoch}, loss = {loss.item()}, WER = {avg_error}, BLEU = {avg_bleu}, CIDER = {avg_cider}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "\n",
    "    # 学習率を表示\n",
    "    print(f\"学習率 clip,         param_groups:\", optimizer.param_groups[0][\"lr\"] )\n",
    "    print(f\"学習率 prop_decoder, param_groups:\", optimizer.param_groups[1][\"lr\"] )\n",
    "    print(f\"学習率 aux_decoder,  param_groups:\", optimizer.param_groups[2][\"lr\"] )\n",
    "    print(f\"学習率 others,       param_groups:\", optimizer.param_groups[3][\"lr\"] )\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_error = np.mean(train_errors )\n",
    "    train_bleu = np.mean(train_bleus )\n",
    "    #train_cider = np.mean(train_ciders )\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train WER: {train_error}')        \n",
    "    print(f'Train BLEU: {train_bleu}')\n",
    "    #print(f'Train CIDER: {train_cider}')\n",
    "\n",
    "    # 検証\n",
    "    with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[検証]')\n",
    "\n",
    "        # 評価モード\n",
    "        model.eval()\n",
    "\n",
    "        val_losses = deque()\n",
    "        val_errors = deque()\n",
    "        val_bleus = deque()\n",
    "        #val_ciders = deque()\n",
    "        for n_batch, (imgs, captions, caption_lengths) in enumerate( pbar ):\n",
    "\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "            #caption_lengths = caption_lengths.to(config.device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # 最後の単語から次を予測する必要はないため最後の単語を除外\n",
    "                ctc_logits, ce_logits = model(imgs, captions )\n",
    "                hypo_ids = torch.argmax( ctc_logits, dim = 2 )\n",
    "                ctc_outputs_lengths = torch.full( size = (ctc_logits.size(0), ), fill_value = ctc_logits.size(1), dtype=torch.long)\n",
    "                ctc_outputs = F.log_softmax( ctc_logits, dim=2 )\n",
    "                \n",
    "                # 損失の計算\n",
    "                # 単語軸が第1軸である必要があるため、転置\n",
    "                ctc_loss = ctc_criterion(ctc_outputs.transpose(0, 1),captions,ctc_outputs_lengths,caption_lengths)\n",
    "                #attention_mask = (~torch.eq( captions[:,1:], tokenizer.pad_token_id )).float()\n",
    "                #attention_mask = None\n",
    "                #kd_logits = kd_model( input_ids = captions, attention_mask = attention_mask )[0]\n",
    "                kd_logits = kd_model( input_ids = captions )[0]\n",
    "                kd_loss = nn.KLDivLoss()(F.log_softmax(ce_logits/config.T, dim=-1),F.softmax(kd_logits/config.T, dim=-1)) * ( config.T * config.T )\n",
    "                loss = config.alpha * ctc_loss + ( 1 - config.alpha ) * kd_loss\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            #total_cider = 0\n",
    "            n2 = 0\n",
    "            for (hypo_id, caption) in zip( hypo_ids, captions ):\n",
    "                hypo_tokens = ctc_simple_decode(hypo_id, tokenizer)\n",
    "                hypo = ' '.join( hypo_tokens )\n",
    "                reference = tokenizer.decode( caption.tolist(), skip_special_tokens = True )\n",
    "                ref_tokens = tokenizer.tokenize( reference )\n",
    "\n",
    "                # 認識誤りを計算\n",
    "                (error, substitute, delete, insert, ref_length) = levenshtein.calculate_error(hypo_tokens, ref_tokens)\n",
    "                    \n",
    "                # 誤り文字数を累積する\n",
    "                total_error += error\n",
    "                # 文字の総数を累積する\n",
    "                total_token_length += ref_length\n",
    "\n",
    "                bleu = bleu_score.sentence_bleu( [reference], hypo, smoothing_function=fn  )\n",
    "                #cider_ = cider(predictions=[hypo], references=[[reference]])['avg_score']\n",
    "        \n",
    "                total_bleu += bleu\n",
    "                #total_cider += cider_\n",
    "                #print(\"hypo:\", ' '.join(hypo))\n",
    "\n",
    "                if n < 1 and n_batch == len( val_loader ) - 1:\n",
    "                    hypo_sentence.append( hypo )\n",
    "                    ref_sentence.append( reference )\n",
    "                        \n",
    "                if n < 1 and n_batch % val_param == 0:\n",
    "                    hypo_sentence1.append( hypo )\n",
    "                    ref_sentence1.append( reference )\n",
    "                    \n",
    "                n += 1\n",
    "                n2 += 1\n",
    "                \n",
    "            avg_error = total_error / total_token_length * 100                    \n",
    "            avg_bleu = total_bleu / n2 * 100\n",
    "            #avg_cider = total_cider / n2\n",
    "\n",
    "            # 学習時の損失をログに書き込み\n",
    "            val_losses.append(loss.item())\n",
    "            val_errors.append( avg_error )\n",
    "            val_bleus.append( avg_bleu )\n",
    "            if len(val_losses) > config.moving_avg:\n",
    "                val_losses.popleft()\n",
    "                val_errors.popleft()\n",
    "                val_bleus.popleft()\n",
    "            mean_loss = torch.Tensor(val_losses).mean().item()\n",
    "            mean_error = torch.Tensor(val_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(val_bleus).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                'loss': mean_loss,\n",
    "                'WER': mean_error,\n",
    "                'BLEU': mean_bleu,\n",
    "            })\n",
    "            # Validation Lossをログに書き込み\n",
    "            with open(val_loss_file, 'a') as f:\n",
    "                print(f'{epoch}, {mean_loss},  {mean_error}, {mean_bleu}', file=f)\n",
    "\n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                print(f'Val epoch = {epoch}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "                    \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence, ref_sentence ):\n",
    "                print(f'Val epoch = {epoch}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "\n",
    "    # Loss 表示\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_error = np.mean( val_errors )\n",
    "    val_bleu = np.mean( val_bleus )\n",
    "    #val_cider = np.mean( val_ciders )\n",
    "    print(f'Validation loss: {val_loss}')\n",
    "    print(f'Validation WER: {val_error}')\n",
    "    print(f'Validation BLEU: {val_bleu}')\n",
    "    #print(f'Validation CIDER: {val_cider}')\n",
    "\n",
    "    # より良い検証結果が得られた場合、モデルを保存\n",
    "    if val_loss < val_loss_best:\n",
    "        val_loss_best = val_loss\n",
    "\n",
    "        # モデルを保存\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'global_step': global_step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,},\n",
    "            f'{config.save_directory}/model_clip_decoder2_kd_nar_best.pth')\n",
    "            \n",
    "    # モデルを保存\n",
    "    torch.save({'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,},\n",
    "        f'{config.save_directory}/model_clip_decoder2_kd_nar_curr.pth')\n",
    "        \n",
    "# モデルを保存\n",
    "torch.save({'epoch': epoch,\n",
    "    'global_step': global_step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'loss': loss,},\n",
    "    f'{config.save_directory}/model_clip_decoder2_kd_nar_final.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10ac0991cf0d4346a2dd950c4e69de74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc134416b78b4e16ab143f7b3c9b4db1",
      "placeholder": "​",
      "style": "IPY_MODEL_3e3ad9c31b8244d9987cb5d09ac80909",
      "value": " 230M/230M [00:03&lt;00:00, 57.2MB/s]"
     }
    },
    "189ddc1a5e674235a5a4c9461ec37bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89ed549065394bad9c015c612b04c1d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f9994c1870c348b0ab983404decb722e",
      "value": "100%"
     }
    },
    "2030796b20a24135908446e8a05b2447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e3ad9c31b8244d9987cb5d09ac80909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741ba7eec1f94e6889152972dbf220c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89ed549065394bad9c015c612b04c1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8d4c88aa714714b864351862297512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_741ba7eec1f94e6889152972dbf220c0",
      "max": 241669177,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be3b477f459a4fdd80b055ef88cf8b52",
      "value": 241669177
     }
    },
    "be3b477f459a4fdd80b055ef88cf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc134416b78b4e16ab143f7b3c9b4db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1785c740b7449584ceec0225c64c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_189ddc1a5e674235a5a4c9461ec37bb7",
       "IPY_MODEL_af8d4c88aa714714b864351862297512",
       "IPY_MODEL_10ac0991cf0d4346a2dd950c4e69de74"
      ],
      "layout": "IPY_MODEL_2030796b20a24135908446e8a05b2447"
     }
    },
    "f9994c1870c348b0ab983404decb722e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
